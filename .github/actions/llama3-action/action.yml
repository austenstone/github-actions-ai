name: 'Llama3 CLI Action'
description: 'Runs Llama3 locally using Ollama and outputs response based on a given prompt.'
inputs:
  prompt:
    description: 'Prompt text input for Llama3'
    required: true
outputs:
  response:
    description: 'Text response from Llama3'
    value: ${{ steps.run-llama3.outputs.response }}

runs:
  using: 'composite'
  steps:
    - name: Cache Ollama installation
      uses: actions/cache@v4
      id: cache-ollama
      with:
        path: |
          ~/.ollama
          /usr/local/bin/ollama
        key: ollama-${{ runner.os }}-${{ hashFiles('**/action.yml') }}
        restore-keys: |
          ollama-${{ runner.os }}-

    - name: Cache Llama3 model
      uses: actions/cache@v4
      id: cache-llama3-model
      with:
        path: ~/.ollama/models
        key: llama3-model-${{ runner.os }}-v1
        restore-keys: |
          llama3-model-${{ runner.os }}-

    - name: Install Ollama
      if: steps.cache-ollama.outputs.cache-hit != 'true'
      shell: bash
      run: |
        curl -fsSL https://ollama.com/install.sh | sh

    - name: Start Ollama server
      shell: bash
      run: |
        ollama serve &
        
        # Wait until ollama server is ready
        timeout=30
        until curl --output /dev/null --silent --head --fail http://127.0.0.1:11434; do
          printf '.'
          sleep 1
          timeout=$((timeout - 1))
          if [ $timeout -le 0 ]; then
            echo "Ollama server failed to start"
            exit 1
          fi
        done

    - name: Pull Llama3 model
      if: steps.cache-llama3-model.outputs.cache-hit != 'true'
      shell: bash
      run: |
        ollama pull llama3

    - name: Run Llama3 with prompt
      id: run-llama3
      shell: bash
      run: |
        RESPONSE=$(ollama run llama3 "${{ inputs.prompt }}" | tr -d '\n')
        echo "response=$RESPONSE" >> "$GITHUB_OUTPUT"