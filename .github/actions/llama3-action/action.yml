name: 'Llama3 CLI Action'
description: 'Runs Llama3 locally using Ollama and outputs response based on a given prompt.'
inputs:
  prompt:
    description: 'Prompt text input for Llama3'
    required: true
outputs:
  response:
    description: 'Text response from Llama3'
    value: ${{ steps.run-llama3.outputs.response }}

runs:
  using: 'composite'
  steps:
    - name: Install Ollama and Llama3
      run: |
        curl -fsSL https://ollama.com/install.sh | sh
        ollama serve &
        
        # Wait until ollama server is ready
        timeout=30
        until curl --output /dev/null --silent --head --fail http://127.0.0.1:11434; do
          printf '.'
          sleep 1
          timeout=$((timeout - 1))
          if [ $timeout -le 0 ]; then
            echo "Ollama server failed to start"
            exit 1
          fi
        done

        # Pull Llama3 model
        ollama pull llama3

    - name: Run Llama3 with prompt
      id: run-llama3
      shell: bash
      run: |
        RESPONSE=$(ollama run llama3 "${{ inputs.prompt }}" | tr -d '\n')
        echo "response=$RESPONSE" >> "$GITHUB_OUTPUT"
